%% ======================= Part 2: Plotting =======================
data = load('ex1data1.txt'); % read comma separated data
X = data(:, 1); y = data(:, 2);
m = length(y); % number of training examples 长度而m是一个具体的数值
plot(X, y, 'rx', 'MarkerSize', 10); % Plot the data
ylabel('Profit in $10,000s'); % Set the yyaxis label
xlabel('Population of City in 10,000s'); % Set the xx axis label


%% =================== Part 3: Cost and Gradient descent ===================
%这个公式中有n+1个参数和n个变量，为了使得公式能够简化一些，引入x0=1，
% 则公式转化为：hθ(x)=θ0*x0+θ1*x1+θ2*x2+...+θn*xn
%任何一个训练实例也都是n+1维的向量
X = [ones(m, 1), data(:,1)]; % Add a column of ones to x
 
%初始化选择合适的参数（parameters）θ0和θ1
theta = zeros(2, 1); % initialize fitting parameters
 
% Some gradient descent settings
                    
iterations = 1500;%迭代数
alpha = 0.01;%学习率
 
fprintf('\nTesting the cost function ...\n')
% compute and display initial cost
J = computeCost(X, y, theta);%代价函数
 
fprintf('With theta = [0 ; 0]\nCost computed = %f\n', J);
fprintf('Expected cost value (approx) 32.07\n');
 
% further testing of the cost function
J = computeCost(X, y, [-1 ; 2]);%代价函数
fprintf('\nWith theta = [-1 ; 2]\nCost computed = %f\n', J);
fprintf('Expected cost value (approx) 54.24\n');
 
fprintf('\nRunning Gradient Descent ...\n')
% run gradient descent
%梯度下降算法
theta = gradientDescent(X, y, theta, alpha, iterations);


% print theta to screen
fprintf('Theta found by gradient descent:\n');
fprintf('%f\n', theta);
fprintf('Expected theta values (approx)\n');

fprintf(' -3.6303\n  1.1664\n\n');
 
hold off % don't overlay any more plots on this figure
% Plot the linear fit
hold on; % keep previous plot visible
plot(X(:,2), X*theta, '-')
legend('Training data', 'Linear regression')
 
% Predict values for population sizes of 35,000 and 70,000
%用X的输入特征来预测房价 这里最终结果需要扩大1000倍
predict1 = [1, 3.5] *theta;

fprintf('For population = 35,000, we predict a profit of %f\n',...
    predict1*10000);

predict2 = [1, 7] * theta;

fprintf('For population = 70,000, we predict a profit of %f\n',...
    predict2*10000);

 
fprintf('Program paused. Press enter to continue.\n');
%%

%% ============= Part 4: Visualizing J(theta_0, theta_1) =============
fprintf('Visualizing J(theta_0, theta_1) ...\n')
 
% Grid over which we will calculate J
theta0_vals = linspace(-10, 10, 100);%生成范围在[-10,10]之间100个点的线性行矢量，即维数为1*100的矩阵
theta1_vals = linspace(-1, 4, 100);%生成范围在[-1,4]之间100个点的线性行矢量，即维数为1*100的矩阵
 
% initialize J_vals to a matrix of 0's
J_vals = zeros(length(theta0_vals), length(theta1_vals));%对应的代价函数值，维数为100*100
 
% Fill out J_vals
for i = 1:length(theta0_vals)      %计算代价函数值
    for j = 1:length(theta1_vals)
	  t = [theta0_vals(i); theta1_vals(j)];
	  J_vals(i,j) = computeCost(X, y, t);
    end
end
 
 
% Because of the way meshgrids work in the surf command, we need to
% transpose J_vals before calling surf, or else the axes will be flipped
J_vals = J_vals';%surface函数的特性，必须进行转置。其实就是因为θ0和θ1要和行列坐标x，y对齐
% Surface plot 3
figure;
surf(theta0_vals, theta1_vals, J_vals)    %绘制表面图
xlabel('\theta_0'); ylabel('\theta_1');
 
% Contour plot
figure;
% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100
contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))
xlabel('\theta_0'); ylabel('\theta_1');
hold on;
plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2);
% 利用正规方程解出向量
theta=pinv(X'*X)*X'*y;


gradientDescent 
function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha
 
% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);%初始化代价函数
%% num_iters为迭代数
for iter = 1:num_iters 
    % ====================== YOUR CODE HERE ======================
    % Instructions: Perform a single gradient step on the parameter vector
    %               theta. 
    %
    % Hint: While debugging, it can be useful to print out the values
    %       of the cost function (computeCost) and gradient here.
    %
    % ============================================================
    %向量化求值
    %假设函数hθ(x) = θ0·x0 + θ1·x1 ，用向量表示成：hθ(x) =Tx
    %其中，下面公式的向量表示就是：[XT · (X·θ - y)]/m，用Matlab表示就是：X'*(X*theta-y) / m
    theta = theta - (alpha/m)*X'*(X*theta-y); % theta 就是用上面的向量表示法的 matlab 语言实现
%    遍历求值
%    for iter = 1:num_iters
%        for j = 1 : n
%            theta_new(j) = theta(j) - (alpha / m) * sum((X * theta - y) .* X(:, j));
%        end
%        theta = theta_new;
 
    % Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);
end

computeCost
function J = computeCost(X, y, theta)
%线性回归算法

% Initialize some useful values
m = length(y); % number of training examples
 
% You need to return the following variables correctly 
J = 0;
 
% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta
%               You should set J to the cost.
h = X*theta - y; %X*theta 为Hypothesis   linear regression
J = 1/(2*m) * sum(h.^2);%Cost Function
 
% =========================================================================
 
end

featureNormalize
function [X_norm, mu, sigma] = featureNormalize(X)
%FEATURENORMALIZE Normalizes the features in X 
%   FEATURENORMALIZE(X) returns a normalized version of X where.
%   the mean value of each feature is 0 and the standard deviation
%   is 1. This is often a good preprocessing step to do when
%   working with learning algorithms.
 
% You need to set these values correctly
X_norm = X;
mu = zeros(1, size(X, 2));
sigma = zeros(1, size(X, 2));
 
% ====================== YOUR CODE HERE ======================
% Instructions: First, for each feature dimension, compute the mean
%               of the feature and subtract it from the dataset,
%               storing the mean value in mu. Next, compute the 
%               standard deviation of each feature and divide
%               each feature by it's standard deviation, storing
%               the standard deviation in sigma. 
%
%               Note that X is a matrix where each column is a 
%               feature and each row is an example. You need 
%               to perform the normalization separately for 
%               each feature. 
%
% Hint: You might find the 'mean' and 'std' functions useful.
%       
mu = mean(X);%均值
sigma = std(X);%标准差
 
%梯度下降法实践1-特征缩放公式
for i = 1:size(X,1)
    X_norm(i, :) = (X(i, :) - mu) ./sigma;
end
% ============================================================
 
end
 


